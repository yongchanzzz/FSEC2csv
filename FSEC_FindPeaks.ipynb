{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongchanzzz/FSEC2csv/blob/main/FSEC_FindPeaks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FSEC Multi-Peak Analysis\n",
        "Upload a .csv file generated by [FSEC2csv](https://colab.research.google.com/github/yongchanzzz/FSEC2csv/blob/main/FSEC2csv.ipynb) and it will calculate the area under curve for each peak using Gaussian deconvolution.\n",
        "\n",
        "This notebook supports fitting **1-5 peaks** with user-defined search regions."
      ],
      "metadata": {
        "id": "ZVnj5ZIdOCkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload CSV file\n",
        "from google.colab import files\n",
        "\n",
        "# Upload CSV file\n",
        "uploaded = files.upload()\n",
        "print(\"\\nFile uploaded successfully!\")"
      ],
      "metadata": {
        "id": "OiC398_lOndi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure Analysis Parameters\n",
        "#@markdown **Basic Parameters**\n",
        "flow_rate = 0.4  # @param {type:\"number\"}\n",
        "\n",
        "#@markdown **Peak Parameters (start_vol, end_vol, width)**\n",
        "\n",
        "#@markdown Enter comma-separated values for each peak. Use None for start_vol and end_vol to deactivate a peak.\n",
        "peak_1 = \"2.15, 2.35, 0\"  # @param {type:\"string\"}\n",
        "peak_2 = \"2.45, 2.65, 0\"  # @param {type:\"string\"}\n",
        "peak_3 = \"None\"      # @param {type:\"string\"}\n",
        "peak_4 = \"None\"      # @param {type:\"string\"}\n",
        "peak_5 = \"None\"      # @param {type:\"string\"}\n",
        "peak_6 = \"None\"      # @param {type:\"string\"}\n",
        "peak_7 = \"None\"      # @param {type:\"string\"}\n",
        "peak_8 = \"None\"      # @param {type:\"string\"}\n",
        "peak_9 = \"None\"      # @param {type:\"string\"}\n",
        "peak_10 = \"None\"     # @param {type:\"string\"}\n",
        "\n",
        "# Process peak parameters\n",
        "peak_inputs = [peak_1, peak_2, peak_3, peak_4, peak_5, peak_6, peak_7, peak_8, peak_9, peak_10]\n",
        "peak_params = {}\n",
        "active_peaks = 0\n",
        "\n",
        "print(f\"Flow rate: {flow_rate} mL/min\")\n",
        "print(\"\\nPeak Configuration:\")\n",
        "\n",
        "\n",
        "for i, peak_input in enumerate(peak_inputs):\n",
        "    peak_num = i + 1\n",
        "\n",
        "    try:\n",
        "        # Check if peak is deactivated\n",
        "        if peak_input.strip().lower() in ['none', 'null', '']:\n",
        "            print(f\"Peak {peak_num}: Deactivated\")\n",
        "            continue\n",
        "\n",
        "        # Parse the comma-separated values\n",
        "        values = [val.strip() for val in peak_input.split(',')]\n",
        "\n",
        "        if len(values) != 3:\n",
        "            print(f\"Peak {peak_num}: Invalid format (expected 3 values or 'None')\")\n",
        "            continue\n",
        "\n",
        "        # Convert to appropriate types\n",
        "        start_vol = float(values[0])\n",
        "        end_vol = float(values[1])\n",
        "        width = float(values[2])\n",
        "\n",
        "        # Add active peak\n",
        "        peak_params[peak_num] = {\n",
        "            'start_vol': start_vol,\n",
        "            'end_vol': end_vol,\n",
        "            'width': width\n",
        "        }\n",
        "        active_peaks += 1\n",
        "        width_str = \"auto-fit\" if width == 0 else f\"fixed ({width})\"\n",
        "        print(f\"Peak {peak_num}: Vol {start_vol:.3f} - {end_vol:.3f}, Width = {width_str}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Peak {peak_num}: Error parsing values - {e}\")\n",
        "\n",
        "print(f\"\\nTotal active peaks: {active_peaks}\")\n",
        "print(\"âœ… Parameters configured successfully!\")\n",
        "print(\"Ready to run analysis!\")"
      ],
      "metadata": {
        "id": "config_params",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Multi-Peak Analysis with Correlation Coefficient\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile, os\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "def gaussian(x, a, b, c):\n",
        "    \"\"\"Single Gaussian function\"\"\"\n",
        "    return a * np.exp(-((x - b)**2) / (2 * c**2))\n",
        "\n",
        "def multi_gaussian(x, *params):\n",
        "    \"\"\"Multi-Gaussian function for n peaks\"\"\"\n",
        "    n_peaks = len(params) // 3\n",
        "    result = np.zeros_like(x)\n",
        "\n",
        "    for i in range(n_peaks):\n",
        "        a, b, c = params[3*i:3*i+3]\n",
        "        result += gaussian(x, a, b, c)\n",
        "\n",
        "    return result\n",
        "\n",
        "def calculate_correlation_profile(time, intensity, full_params, window_size=20):\n",
        "    \"\"\"Calculate correlation coefficient across the entire volume range using a sliding window\"\"\"\n",
        "    if len(time) < window_size:\n",
        "        window_size = len(time) // 2\n",
        "\n",
        "    correlation_values = []\n",
        "    time_centers = []\n",
        "\n",
        "    # Calculate predicted values for entire dataset\n",
        "    y_pred_full = multi_gaussian(time, *full_params)\n",
        "\n",
        "    # Sliding window approach\n",
        "    for i in range(window_size//2, len(time) - window_size//2):\n",
        "        start_idx = i - window_size//2\n",
        "        end_idx = i + window_size//2\n",
        "\n",
        "        # Extract window data\n",
        "        y_actual = intensity[start_idx:end_idx]\n",
        "        y_pred = y_pred_full[start_idx:end_idx]\n",
        "\n",
        "        # Calculate correlation coefficient for this window\n",
        "        try:\n",
        "            corr, _ = pearsonr(y_actual, y_pred)\n",
        "            if np.isnan(corr):\n",
        "                corr = 0\n",
        "        except:\n",
        "            corr = 0\n",
        "\n",
        "        correlation_values.append(corr)\n",
        "        time_centers.append(time[i])\n",
        "\n",
        "    return np.array(time_centers), np.array(correlation_values)\n",
        "\n",
        "def find_peak_in_region(time, intensity, start_vol, end_vol, flow_rate):\n",
        "    \"\"\"Find peak maximum in specified volume region with peak detection\"\"\"\n",
        "    start_time = start_vol / flow_rate\n",
        "    end_time = end_vol / flow_rate\n",
        "\n",
        "    mask = (time >= start_time) & (time <= end_time)\n",
        "    if not mask.any():\n",
        "        return None\n",
        "\n",
        "    t, i = time[mask], intensity[mask]\n",
        "\n",
        "    # Use peak detection for better initial guess\n",
        "    try:\n",
        "        peaks, _ = find_peaks(i, height=i.max() * 0.1)\n",
        "        if len(peaks) > 0:\n",
        "            best_peak = max(peaks, key=lambda p: i.iloc[p])\n",
        "            return t.iloc[best_peak], i.iloc[best_peak]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Fallback to maximum\n",
        "    idx = np.argmax(i)\n",
        "    return t.iloc[idx], i.iloc[idx]\n",
        "\n",
        "def fit_multi_peak(time, intensity, peak_params, flow_rate):\n",
        "    \"\"\"Enhanced multi-peak fitting with better convergence\"\"\"\n",
        "    x = time.values\n",
        "    y = intensity.values\n",
        "\n",
        "    # Find overall fitting range\n",
        "    all_starts = [p['start_vol']/flow_rate for p in peak_params.values()]\n",
        "    all_ends = [p['end_vol']/flow_rate for p in peak_params.values()]\n",
        "    fit_start = min(all_starts)\n",
        "    fit_end = max(all_ends)\n",
        "\n",
        "    mask = (x >= fit_start) & (x <= fit_end)\n",
        "    x_fit, y_fit = x[mask], y[mask]\n",
        "\n",
        "    if len(x_fit) == 0:\n",
        "        return None\n",
        "\n",
        "    # Normalize y-values to improve numerical stability\n",
        "    y_max = np.max(y_fit)\n",
        "    y_normalized = y_fit / y_max\n",
        "\n",
        "    # Prepare initial guesses and bounds with better estimates\n",
        "    p0 = []\n",
        "    lower_bounds = []\n",
        "    upper_bounds = []\n",
        "    fitted_params = []\n",
        "    fixed_params = []\n",
        "\n",
        "    for peak_num, params in peak_params.items():\n",
        "        # Find peak in region\n",
        "        peak_info = find_peak_in_region(time, intensity,\n",
        "                                       params['start_vol'], params['end_vol'], flow_rate)\n",
        "        if peak_info is None:\n",
        "            return None\n",
        "\n",
        "        b_guess, a_guess = peak_info\n",
        "        a_guess_normalized = a_guess / y_max  # Normalize amplitude guess\n",
        "\n",
        "        if params['width'] > 0:\n",
        "            # Fixed width - only fit amplitude and position\n",
        "            fixed_params.append(f\"c{peak_num}={params['width']}\")\n",
        "        else:\n",
        "            # Auto-fit width with better initial guess\n",
        "            c_guess = (params['end_vol'] - params['start_vol']) / (8 * flow_rate)  # More conservative initial guess\n",
        "            p0.append(c_guess)\n",
        "            lower_bounds.append(c_guess * 0.1)  # Minimum width\n",
        "            upper_bounds.append(c_guess * 20)   # Maximum width\n",
        "            fitted_params.append(f\"c{peak_num}\")\n",
        "\n",
        "        # Always fit amplitude and position\n",
        "        p0.extend([a_guess_normalized, b_guess])\n",
        "        lower_bounds.extend([0.001, params['start_vol']/flow_rate])  # Small minimum amplitude\n",
        "        upper_bounds.extend([5.0, params['end_vol']/flow_rate])  # Reasonable upper bound for normalized amplitude\n",
        "        fitted_params.extend([f\"a{peak_num}\", f\"b{peak_num}\"])\n",
        "\n",
        "    # Create fitting function that handles fixed widths\n",
        "    def fitting_function(x, *fit_params):\n",
        "        full_params = []\n",
        "        fit_idx = 0\n",
        "\n",
        "        for peak_num, params in peak_params.items():\n",
        "            if params['width'] > 0:\n",
        "                # Fixed width\n",
        "                a = fit_params[fit_idx]\n",
        "                b = fit_params[fit_idx + 1]\n",
        "                c = params['width'] / flow_rate  # Convert to time units\n",
        "                fit_idx += 2\n",
        "            else:\n",
        "                # Fitted width\n",
        "                c = fit_params[fit_idx]\n",
        "                a = fit_params[fit_idx + 1]\n",
        "                b = fit_params[fit_idx + 2]\n",
        "                fit_idx += 3\n",
        "\n",
        "            full_params.extend([a, b, c])\n",
        "\n",
        "        return multi_gaussian(x, *full_params)\n",
        "\n",
        "    # Try multiple optimization strategies with different settings\n",
        "    strategies = [\n",
        "        {'maxfev': 20000, 'method': 'lm'},\n",
        "        {'maxfev': 30000, 'method': 'trf'},\n",
        "        {'maxfev': 50000, 'method': 'dogbox'},\n",
        "        {'maxfev': 100000, 'method': 'trf', 'ftol': 1e-12, 'xtol': 1e-12}\n",
        "    ]\n",
        "\n",
        "    popt = None\n",
        "    last_error = None\n",
        "\n",
        "    for i, strategy in enumerate(strategies):\n",
        "        try:\n",
        "            popt, pcov = curve_fit(\n",
        "                fitting_function, x_fit, y_normalized,\n",
        "                p0=p0,\n",
        "                bounds=(lower_bounds, upper_bounds),\n",
        "                **strategy\n",
        "            )\n",
        "            print(f\"    Convergence achieved with strategy {i+1}\")\n",
        "            break  # Success!\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            if i < len(strategies) - 1:\n",
        "                print(f\"    Strategy {i+1} failed, trying next...\")\n",
        "            continue\n",
        "\n",
        "    if popt is None:\n",
        "        print(f\"    âš ï¸  All optimization strategies failed. Last error: {last_error}\")\n",
        "        return None\n",
        "\n",
        "    # Reconstruct full parameters and denormalize\n",
        "    full_params = []\n",
        "    fit_idx = 0\n",
        "\n",
        "    for peak_num, params in peak_params.items():\n",
        "        if params['width'] > 0:\n",
        "            a = popt[fit_idx] * y_max  # Denormalize amplitude\n",
        "            b = popt[fit_idx + 1]\n",
        "            c = params['width'] / flow_rate\n",
        "            fit_idx += 2\n",
        "        else:\n",
        "            c = popt[fit_idx]\n",
        "            a = popt[fit_idx + 1] * y_max  # Denormalize amplitude\n",
        "            b = popt[fit_idx + 2]\n",
        "            fit_idx += 3\n",
        "\n",
        "        full_params.extend([a, b, c])\n",
        "\n",
        "    return full_params, x_fit, y_fit, fitted_params, fixed_params\n",
        "\n",
        "def main():\n",
        "    # Check if parameters are set\n",
        "    if not peak_params:\n",
        "        print(\"âŒ Error: Please configure peak parameters first!\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    fname = next(iter(uploaded.keys()))\n",
        "    df = pd.read_csv(fname)\n",
        "    time = df['time']\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs('plots', exist_ok=True)\n",
        "    results = []\n",
        "\n",
        "    print(f\"Processing {len(df.columns) - 1} series with {active_peaks} peaks...\")\n",
        "    print(f\"Using enhanced fitting algorithm with correlation coefficient analysis...\")\n",
        "\n",
        "    failed_series = []\n",
        "\n",
        "    for col in df.columns.drop('time'):\n",
        "        print(f\"\\nðŸ“Š Processing '{col}'...\")\n",
        "        intensity = df[col]\n",
        "\n",
        "        # Fit peaks\n",
        "        fit_result = fit_multi_peak(time, intensity, peak_params, flow_rate)\n",
        "\n",
        "        if fit_result is None:\n",
        "            print(f\"    âŒ Skipped '{col}': fitting failed\")\n",
        "            failed_series.append(col)\n",
        "            continue\n",
        "\n",
        "        full_params, x_fit, y_fit, fitted_params, fixed_params = fit_result\n",
        "\n",
        "        # Calculate areas and positions\n",
        "        series_result = {'Series': col}\n",
        "\n",
        "        for i in range(active_peaks):\n",
        "            peak_num = i + 1\n",
        "            a, b, c = full_params[3*i:3*i+3]\n",
        "\n",
        "            area = a * c * np.sqrt(2 * np.pi)\n",
        "            position = b * flow_rate\n",
        "\n",
        "            series_result.update({\n",
        "                f'Peak{peak_num}_Position_Vol': position,\n",
        "                f'Peak{peak_num}_Area': area,\n",
        "                f'Peak{peak_num}_Height': a,\n",
        "                f'Peak{peak_num}_Width': c * flow_rate  # Convert back to volume units\n",
        "            })\n",
        "\n",
        "        # Calculate correlation coefficient for ENTIRE dataset (not just fitting range)\n",
        "        y_pred_full = multi_gaussian(time.values, *full_params)\n",
        "\n",
        "        # Overall correlation for entire dataset\n",
        "        try:\n",
        "            overall_corr, _ = pearsonr(intensity.values, y_pred_full)\n",
        "            if np.isnan(overall_corr):\n",
        "                overall_corr = 0\n",
        "        except:\n",
        "            overall_corr = 0\n",
        "\n",
        "        # Also calculate RMSE for the fitting range (for comparison)\n",
        "        y_pred_fit = multi_gaussian(x_fit, *full_params)\n",
        "        rmse = np.sqrt(np.mean((y_fit - y_pred_fit)**2))\n",
        "\n",
        "        series_result.update({\n",
        "            'RMSE_FitRange': rmse,\n",
        "            'Correlation_Overall': overall_corr\n",
        "        })\n",
        "\n",
        "        results.append(series_result)\n",
        "\n",
        "        # Calculate correlation profile across entire volume range\n",
        "        time_centers, corr_profile = calculate_correlation_profile(time.values, intensity.values, full_params)\n",
        "        vol_centers = time_centers * flow_rate\n",
        "\n",
        "        # Create enhanced plot with correlation profile\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), height_ratios=[3, 1])\n",
        "\n",
        "        # Main plot (top)\n",
        "        vol = time * flow_rate\n",
        "        ax1.plot(vol, intensity, '-', label='Data', alpha=0.7)\n",
        "        ax1.plot(vol, multi_gaussian(time, *full_params),\n",
        "                '--', linewidth=2, label='Total Fit', color='red')\n",
        "\n",
        "        # Plot individual peaks\n",
        "        colors = ['orange', 'green', 'blue', 'purple', 'brown']\n",
        "        for i in range(active_peaks):\n",
        "            a, b, c = full_params[3*i:3*i+3]\n",
        "            ax1.plot(vol, gaussian(time, a, b, c),\n",
        "                    ':', color=colors[i % len(colors)], linewidth=2,\n",
        "                    label=f'Peak {i+1} (Area: {a * c * np.sqrt(2 * np.pi):.2f})')\n",
        "\n",
        "        ax1.set_ylabel('Intensity')\n",
        "        ax1.set_title(f'Multi-Peak Deconvolution â€” {col}\\nRMSE: {rmse:.4f}, Overall Correlation: {overall_corr:.4f}')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Correlation profile plot (bottom)\n",
        "        ax2.plot(vol_centers, corr_profile, '-', color='black', linewidth=2, label='Correlation Profile')\n",
        "        ax2.axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='Corr=0.95')\n",
        "        ax2.axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='Corr=0.90')\n",
        "        ax2.axhline(y=0.80, color='red', linestyle='--', alpha=0.7, label='Corr=0.80')\n",
        "\n",
        "        # Highlight peak regions\n",
        "        for i, (peak_num, params) in enumerate(peak_params.items()):\n",
        "            ax2.axvspan(params['start_vol'], params['end_vol'],\n",
        "                       alpha=0.2, color=colors[i % len(colors)],\n",
        "                       label=f'Peak {peak_num} region')\n",
        "\n",
        "        ax2.set_xlabel('Volume (mL)')\n",
        "        ax2.set_ylabel('Correlation (Local Fit Quality)')\n",
        "        ax2.set_ylim(-0.1, 1.1)\n",
        "        ax2.legend(loc='upper right', fontsize=8)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'plots/fit_{col}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Calculate additional correlation statistics\n",
        "        mean_corr = np.mean(corr_profile)\n",
        "        min_corr = np.min(corr_profile)\n",
        "        good_fit_percentage = np.sum(corr_profile > 0.9) / len(corr_profile) * 100\n",
        "\n",
        "        print(f\"    âœ… Success! RMSE: {rmse:.4f}, Overall Correlation: {overall_corr:.4f}\")\n",
        "        print(f\"    ðŸ“ˆ Corr Profile: Mean={mean_corr:.3f}, Min={min_corr:.3f}, >0.9: {good_fit_percentage:.1f}%\")\n",
        "\n",
        "        # Add correlation profile statistics to results\n",
        "        series_result.update({\n",
        "            'Correlation_Profile_Mean': mean_corr,\n",
        "            'Correlation_Profile_Min': min_corr,\n",
        "            'Correlation_Profile_GoodFit_Percent': good_fit_percentage\n",
        "        })\n",
        "\n",
        "    # Save results\n",
        "    if results:\n",
        "        res_df = pd.DataFrame(results)\n",
        "        res_df.to_csv('peak_areas.csv', index=False)\n",
        "        print(f\"ðŸ“Š Results saved for {len(results)} series\")\n",
        "\n",
        "        if failed_series:\n",
        "            print(f\"âš ï¸  Failed series ({len(failed_series)}): {', '.join(failed_series)}\")\n",
        "    else:\n",
        "        print(\"\\nâŒ No results to save\")\n",
        "        return\n",
        "\n",
        "    # Create enhanced detailed report\n",
        "    with open('analysis_report.txt', 'w') as f:\n",
        "        f.write(\"=== Multi-Peak Chromatogram Analysis Report ===\\n\")\n",
        "        f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Input file: {fname}\\n\")\n",
        "        f.write(f\"Flow rate: {flow_rate} mL/min\\n\")\n",
        "        f.write(f\"Number of peaks fitted: {active_peaks}\\n\")\n",
        "        f.write(f\"Total series processed: {len(df.columns) - 1}\\n\")\n",
        "        f.write(f\"Successful fits: {len(results)}\\n\")\n",
        "        f.write(f\"Failed fits: {len(failed_series)}\\n\\n\")\n",
        "\n",
        "        f.write(\"=== Peak Search Parameters ===\\n\")\n",
        "        for peak_num, params in peak_params.items():\n",
        "            f.write(f\"Peak {peak_num}:\\n\")\n",
        "            f.write(f\"- Search volume range: {params['start_vol']:.3f} - {params['end_vol']:.3f} mL\\n\")\n",
        "            f.write(f\"- Width constraint: {'Fixed at ' + str(params['width']) if params['width'] > 0 else 'Auto-fitted'}\\n\")\n",
        "\n",
        "        f.write(\"\\n=== Correlation Coefficient Analysis ===\\n\")\n",
        "        f.write(\"Overall correlation calculated across ENTIRE volume range (not just fitting regions)\\n\")\n",
        "        f.write(\"Local correlation profiles calculated using sliding window approach\\n\")\n",
        "        f.write(\"Correlation coefficient is more robust to baseline shifts and scaling issues\\n\")\n",
        "        f.write(\"Reference lines: Corr>0.95 (excellent), Corr>0.90 (good), Corr>0.80 (acceptable)\\n\\n\")\n",
        "\n",
        "        f.write(\"Software packages used:\\n\")\n",
        "        f.write(\"- numpy (numerical operations)\\n\")\n",
        "        f.write(\"- scipy.optimize.curve_fit (non-linear fitting)\\n\")\n",
        "        f.write(\"- scipy.signal.find_peaks (peak detection)\\n\")\n",
        "        f.write(\"- scipy.stats.pearsonr (correlation coefficient)\\n\")\n",
        "        f.write(\"- matplotlib.pyplot (plotting)\\n\")\n",
        "        f.write(\"- pandas (data handling)\\n\")\n",
        "\n",
        "        f.write(\"\\nMathematical model:\\n\")\n",
        "        f.write(\"- Single Gaussian: G(x; a,b,c) = a * exp(-((x-b)Â²)/(2cÂ²))\\n\")\n",
        "        f.write(f\"- Multi-peak model: f(x) = Î£ G(x; a_i,b_i,c_i) for i=1 to {active_peaks}\\n\")\n",
        "        f.write(\"- Peak area = a * c * âˆš(2Ï€)\\n\")\n",
        "        f.write(\"- Overall correlation: Pearson r between entire observed and predicted datasets\\n\")\n",
        "        f.write(\"- Local correlation: Pearson r calculated using sliding window of 20 data points\\n\\n\")\n",
        "\n",
        "        if results:\n",
        "            f.write(\"=== Fit Quality Summary ===\\n\")\n",
        "            rmse_values = [r['RMSE_FitRange'] for r in results]\n",
        "            corr_values = [r['Correlation_Overall'] for r in results]\n",
        "            corr_mean_values = [r['Correlation_Profile_Mean'] for r in results]\n",
        "            corr_min_values = [r['Correlation_Profile_Min'] for r in results]\n",
        "            good_fit_values = [r['Correlation_Profile_GoodFit_Percent'] for r in results]\n",
        "\n",
        "            f.write(f\"Fitting range statistics:\\n\")\n",
        "            f.write(f\"  Average RMSE: {np.mean(rmse_values):.4f}\\n\")\n",
        "            f.write(f\"  RMSE range: {np.min(rmse_values):.4f} - {np.max(rmse_values):.4f}\\n\\n\")\n",
        "\n",
        "            f.write(f\"Overall correlation statistics (entire volume range):\\n\")\n",
        "            f.write(f\"  Average correlation: {np.mean(corr_values):.4f}\\n\")\n",
        "            f.write(f\"  Correlation range: {np.min(corr_values):.4f} - {np.max(corr_values):.4f}\\n\\n\")\n",
        "\n",
        "            f.write(f\"Local correlation profile statistics:\\n\")\n",
        "            f.write(f\"  Average mean correlation: {np.mean(corr_mean_values):.4f}\\n\")\n",
        "            f.write(f\"  Average minimum correlation: {np.mean(corr_min_values):.4f}\\n\")\n",
        "            f.write(f\"  Average good fit percentage (Corr>0.9): {np.mean(good_fit_values):.1f}%\\n\\n\")\n",
        "\n",
        "            f.write(\"Individual series fit quality:\\n\")\n",
        "            for result in results:\n",
        "                f.write(f\"  {result['Series']}: RMSE={result['RMSE_FitRange']:.4f}, \")\n",
        "                f.write(f\"Overall_Corr={result['Correlation_Overall']:.4f}, \")\n",
        "                f.write(f\"Mean_Corr={result['Correlation_Profile_Mean']:.3f}, \")\n",
        "                f.write(f\"Good_fit={result['Correlation_Profile_GoodFit_Percent']:.1f}%\\n\")\n",
        "\n",
        "        if failed_series:\n",
        "            f.write(\"\\n=== Failed Series ===\\n\")\n",
        "            for series in failed_series:\n",
        "                f.write(f\"  {series}\\n\")\n",
        "\n",
        "        f.write(\"\\n=== End of Report ===\\n\")\n",
        "\n",
        "    # Create downloadable zip file\n",
        "    zip_name = f\"FSEC_FindPeaks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # Add results and report\n",
        "        zipf.write('peak_areas.csv')\n",
        "        zipf.write('analysis_report.txt')\n",
        "\n",
        "        # Add all plots\n",
        "        for filename in os.listdir('plots'):\n",
        "            if filename.endswith('.png'):\n",
        "                zipf.write(os.path.join('plots', filename))\n",
        "\n",
        "    print(f\"ðŸ“¦ Downloading results package: {zip_name}\")\n",
        "    files.download(zip_name)\n",
        "\n",
        "    print(\"ðŸŽ‰ Enhanced analysis with correlation coefficient profiling complete!\")\n",
        "    print(f\"Results include:\")\n",
        "    print(f\"  - peak_areas.csv: Quantitative results with correlation statistics\")\n",
        "    print(f\"  - analysis_report.txt: Detailed analysis report with correlation profiling\")\n",
        "    print(f\"  - {len(results)} enhanced plots with correlation quality profiles\")\n",
        "\n",
        "    if failed_series:\n",
        "        print(f\"Troubleshooting tips for failed series:\")\n",
        "        print(f\"  - Check if peak search ranges are appropriate\")\n",
        "        print(f\"  - Consider using fixed widths for difficult peaks\")\n",
        "        print(f\"  - Verify data quality and signal-to-noise ratio\")\n",
        "        print(f\"  - Use correlation profile to identify problematic regions\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7CxqpvYuMsMQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}